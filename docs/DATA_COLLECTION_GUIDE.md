# Training Data Collection Guide

## Overview

This guide provides structured templates for collecting real training data for the Keyboard Suggestions AI. Follow these formats to ensure data quality and consistency.

**Important**: Before collecting large datasets, **test with minimal data first** to ensure the model learns correctly. See [`TEST_ROADMAP.md`](TEST_ROADMAP.md) for testing strategy.

---

## Quick Start: Test Data (Start Here!)

Before collecting thousands of sentences, **validate the model with minimal test data**.

### Minimal Test Dataset (15 sentences)

Create `data/test/minimal_train.txt`:

```
I am happy
I am sad
I am tired
I love you
I love pizza
I love coding
you are great
you are awesome
you are amazing
thank you very much
thank you so much
how are you
how are you doing
what are you doing
what do you think
```

**Purpose**: Verify model learns patterns:
- "I am" â†’ should predict `["happy", "sad", "tired"]`
- "I love" â†’ should predict `["you", "pizza", "coding"]`
- "you are" â†’ should predict `["great", "awesome", "amazing"]`

**Expected Results**:
- âœ… Different inputs â†’ Different predictions
- âœ… Predictions match training data
- âœ… No crashes on unknown input

**If this works**, scale to 100 â†’ 1,000 â†’ 10,000 sentences.

**If this fails**, model architecture needs fixing before collecting more data.

See [`TEST_ROADMAP.md`](TEST_ROADMAP.md) for complete testing guide.

---

## Data Requirements

### Minimum Dataset Size
- **English**: 10,000+ sentences (target: 50,000+)
- **Japanese**: 10,000+ sentences (target: 50,000+)
- **Total**: 20,000+ sentences minimum

### Quality Guidelines
- Natural, conversational text
- Include emoji and symbols
- Mix of short and long sentences
- Diverse vocabulary
- Real-world typing patterns

---

## 1. English Training Data (Casual/Social Media Style)

### Format
One sentence per line, UTF-8 encoding, with emoji preserved.

### Example File: `english_casual.txt`

```
I'm going to the store later
wanna grab some coffee?
that's so cool!
lol I can't believe it ğŸ˜‚
thanks for your help!
see you tomorrow ğŸ‘‹
I love this song â¤ï¸
omg that's amazing
btw I'll be there at 5
gonna be a great day
check this out ğŸ‘€
I'm so excited ğŸ‰
can't wait to see you
that was hilarious
you're the best
let me know when you're free
I'm on my way
be right back
talk to you later
have a great weekend
good morning everyone â˜€ï¸
congrats on the new job ğŸŠ
happy birthday! ğŸ‚
thinking of you ğŸ’­
miss you tons
how's it going?
what's up?
nothing much, you?
sounds good to me
perfect, see you then
```

### Sources to Collect From
1. **Twitter/X**: Public tweets (casual conversation)
2. **Reddit**: Comments from casual subreddits
3. **SMS/iMessage**: Personal messages (anonymized)
4. **Chat apps**: Discord, Slack public channels
5. **Movie subtitles**: Conversational dialogue
6. **Blog comments**: Informal writing

### Collection Script Example

```python
# english_data_collector.py
import json

def collect_english_casual():
    """
    Template for collecting English casual text.
    Replace with your actual data source.
    """
    sentences = []
    
    # Example: Read from your data source
    # with open('your_source.txt', 'r', encoding='utf-8') as f:
    #     for line in f:
    #         sentence = clean_sentence(line)
    #         if is_valid_casual_english(sentence):
    #             sentences.append(sentence)
    
    # Save to training file
    with open('data/raw/english_casual.txt', 'w', encoding='utf-8') as f:
        for sentence in sentences:
            f.write(sentence + '\n')
    
    return len(sentences)

def is_valid_casual_english(text):
    """Validate casual English text"""
    # Must be 3-50 words
    words = text.split()
    if len(words) < 3 or len(words) > 50:
        return False
    
    # Should contain mostly English characters
    ascii_ratio = sum(ord(c) < 128 for c in text) / len(text)
    if ascii_ratio < 0.7:
        return False
    
    return True
```

---

## 2. Japanese Training Data (Polite/Formal Style)

### Format
One sentence per line, UTF-8 encoding, polite forms preferred.

### Example File: `japanese_polite.txt`

```
ä»Šæ—¥ã¯ã„ã„å¤©æ°—ã§ã™ã­
ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™
ãŠç–²ã‚Œæ§˜ã§ã™
ã‚ˆã‚ã—ããŠé¡˜ã„ã—ã¾ã™
ã™ã¿ã¾ã›ã‚“ãŒã€å°‘ã€…ãŠå¾…ã¡ãã ã•ã„
ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™
ã„ãŸã ãã¾ã™
å¤±ç¤¼ã—ã¾ã™
ãŠä¸–è©±ã«ãªã£ã¦ãŠã‚Šã¾ã™
ã”ç¢ºèªãã ã•ã„
æ‰¿çŸ¥ã„ãŸã—ã¾ã—ãŸ
æã‚Œå…¥ã‚Šã¾ã™ãŒ
ãŠå¾…ãŸã›ã„ãŸã—ã¾ã—ãŸ
ã©ã†ãã‚ˆã‚ã—ããŠé¡˜ã„ã„ãŸã—ã¾ã™
ã”é€£çµ¡ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™
ãŠå¿™ã—ã„ã¨ã“ã‚ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“
ãŠæ‰‹æ•°ã‚’ãŠã‹ã‘ã—ã¾ã™
ã”ç†è§£ã„ãŸã ã‘ã¾ã™ã¨å¹¸ã„ã§ã™
ä½•å’ã‚ˆã‚ã—ããŠé¡˜ã„ç”³ã—ä¸Šã’ã¾ã™
ãŠè¿”äº‹ãŠå¾…ã¡ã—ã¦ãŠã‚Šã¾ã™
æœ¬æ—¥ã¯ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ
ã¾ãŸæ˜æ—¥ãŠä¼šã„ã—ã¾ã—ã‚‡ã†
ãŠæ°—ã‚’ã¤ã‘ã¦ãŠå¸°ã‚Šãã ã•ã„
è‰¯ã„é€±æœ«ã‚’ãŠéã”ã—ãã ã•ã„
ãŠå¤§äº‹ã«ãªã•ã£ã¦ãã ã•ã„
```

### Sources to Collect From
1. **Business emails**: Formal Japanese correspondence
2. **Customer service**: Polite responses
3. **News articles**: Formal writing
4. **Corporate websites**: About pages, announcements
5. **Japanese textbooks**: Polite conversation examples
6. **NHK transcripts**: Formal broadcast language

### Collection Script Example

```python
# japanese_data_collector.py

def collect_japanese_polite():
    """
    Template for collecting Japanese polite text.
    """
    sentences = []
    
    # Example: Read from your data source
    # with open('your_japanese_source.txt', 'r', encoding='utf-8') as f:
    #     for line in f:
    #         sentence = clean_sentence(line)
    #         if is_valid_polite_japanese(sentence):
    #             sentences.append(sentence)
    
    # Save to training file
    with open('data/raw/japanese_polite.txt', 'w', encoding='utf-8') as f:
        for sentence in sentences:
            f.write(sentence + '\n')
    
    return len(sentences)

def is_valid_polite_japanese(text):
    """Validate polite Japanese text"""
    # Must contain Japanese characters
    has_japanese = any('\u3040' <= c <= '\u30ff' or '\u4e00' <= c <= '\u9faf' 
                       for c in text)
    if not has_japanese:
        return False
    
    # Prefer polite forms (ã§ã™ã€ã¾ã™ endings)
    polite_endings = ['ã§ã™', 'ã¾ã™', 'ã¾ã—ãŸ', 'ã¾ã›ã‚“', 'ã”ã–ã„ã¾ã™']
    has_polite = any(text.endswith(ending) for ending in polite_endings)
    
    # Length check
    if len(text) < 5 or len(text) > 200:
        return False
    
    return True
```

---

## 3. Emoji-Rich Training Data

### Format
Sentences with natural emoji usage.

### Example File: `emoji_rich.txt`

```
I love you â¤ï¸
good morning â˜€ï¸
congratulations ğŸ‰
good night ğŸ˜´
that's so funny ğŸ˜‚
thinking of you ğŸ¤”
you're amazing â­
happy birthday ğŸ‚
good luck ğŸ€
thank you so much ğŸ™
I'm so happy ğŸ˜Š
miss you ğŸ’•
you got this ğŸ’ª
feeling great today ğŸ˜
let's celebrate ğŸ¥³
sending hugs ğŸ¤—
coffee time â˜•
pizza night ğŸ•
beach day ğŸ–ï¸
study time ğŸ“š
music lover ğŸµ
movie night ğŸ¬
workout done ğŸ’ª
travel time âœˆï¸
foodie life ğŸ”
```

### Collection Tips
- Look for social media posts with emoji
- Analyze emoji frequency in your target audience
- Include common emoji combinations
- Preserve emoji positioning (beginning, middle, end)

---

## 4. Mixed Language Data (Code-Switching)

### Format
Sentences mixing English and Japanese naturally.

### Example File: `mixed_language.txt`

```
æ±äº¬ is amazing
I love å¯¿å¸
let's go to æ¸‹è°·
this is so ç¾å‘³ã—ã„
meeting at 3æ™‚
see you in æ–°å®¿
I'm learning æ—¥æœ¬èª
that's so å¯æ„›ã„
working in Tokyo ã‚ªãƒ•ã‚£ã‚¹
my favorite å±…é…’å±‹
weekend in äº¬éƒ½
studying at å¤§å­¦
shopping in åŸå®¿
living in Japan is great
I love Japanese æ–‡åŒ–
```

### Collection Sources
- Bilingual social media users
- Language learning forums
- Expat communities
- International business communications

---

## 5. Data Cleaning Pipeline

### Script: `clean_training_data.py`

```python
import re
from pathlib import Path

class DataCleaner:
    """Clean and validate training data"""
    
    def __init__(self):
        self.min_length = 3  # words
        self.max_length = 50  # words
    
    def clean_text(self, text):
        """Clean a single text line"""
        # Remove excessive whitespace
        text = re.sub(r'\s+', ' ', text)
        
        # Remove URLs
        text = re.sub(r'http\S+|www\.\S+', '', text)
        
        # Remove email addresses
        text = re.sub(r'\S+@\S+', '', text)
        
        # Remove excessive punctuation
        text = re.sub(r'([!?.]){3,}', r'\1\1', text)
        
        # Strip and normalize
        text = text.strip()
        
        return text
    
    def is_valid(self, text, language='en'):
        """Validate text quality"""
        if not text:
            return False
        
        # Length check
        words = text.split()
        if len(words) < self.min_length or len(words) > self.max_length:
            return False
        
        # Language-specific validation
        if language == 'en':
            return self._is_valid_english(text)
        elif language == 'ja':
            return self._is_valid_japanese(text)
        
        return True
    
    def _is_valid_english(self, text):
        """Validate English text"""
        # Should be mostly ASCII
        ascii_ratio = sum(ord(c) < 128 for c in text) / len(text)
        return ascii_ratio > 0.6
    
    def _is_valid_japanese(self, text):
        """Validate Japanese text"""
        # Must contain Japanese characters
        return any('\u3040' <= c <= '\u30ff' or '\u4e00' <= c <= '\u9faf' 
                   for c in text)
    
    def process_file(self, input_file, output_file, language='en'):
        """Process entire file"""
        valid_lines = []
        total_lines = 0
        
        with open(input_file, 'r', encoding='utf-8') as f:
            for line in f:
                total_lines += 1
                cleaned = self.clean_text(line)
                
                if self.is_valid(cleaned, language):
                    valid_lines.append(cleaned)
        
        # Write cleaned data
        with open(output_file, 'w', encoding='utf-8') as f:
            for line in valid_lines:
                f.write(line + '\n')
        
        print(f"Processed {input_file}:")
        print(f"  Total lines: {total_lines}")
        print(f"  Valid lines: {len(valid_lines)}")
        print(f"  Kept: {len(valid_lines)/total_lines*100:.1f}%")
        
        return len(valid_lines)

# Usage
if __name__ == "__main__":
    cleaner = DataCleaner()
    
    # Clean English data
    cleaner.process_file(
        'data/raw/english_casual.txt',
        'data/processed/english_clean.txt',
        language='en'
    )
    
    # Clean Japanese data
    cleaner.process_file(
        'data/raw/japanese_polite.txt',
        'data/processed/japanese_clean.txt',
        language='ja'
    )
```

---

## 6. Data Statistics & Validation

### Script: `validate_dataset.py`

```python
import json
from collections import Counter
from pathlib import Path

def analyze_dataset(file_path, language='en'):
    """Analyze training dataset statistics"""
    
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = [line.strip() for line in f if line.strip()]
    
    # Basic stats
    total_sentences = len(lines)
    total_words = sum(len(line.split()) for line in lines)
    total_chars = sum(len(line) for line in lines)
    
    # Word length distribution
    word_lengths = [len(line.split()) for line in lines]
    
    # Character frequency
    all_text = ''.join(lines)
    char_freq = Counter(all_text)
    
    # Emoji count
    emoji_count = sum(1 for c in all_text 
                     if ord(c) > 0x1F300)
    
    stats = {
        'file': str(file_path),
        'language': language,
        'total_sentences': total_sentences,
        'total_words': total_words,
        'total_characters': total_chars,
        'avg_words_per_sentence': total_words / total_sentences,
        'avg_chars_per_sentence': total_chars / total_sentences,
        'min_words': min(word_lengths),
        'max_words': max(word_lengths),
        'emoji_count': emoji_count,
        'unique_characters': len(char_freq),
        'most_common_chars': char_freq.most_common(20)
    }
    
    # Print report
    print(f"\n{'='*60}")
    print(f"Dataset Analysis: {file_path.name}")
    print(f"{'='*60}")
    print(f"Language: {language}")
    print(f"Total sentences: {stats['total_sentences']:,}")
    print(f"Total words: {stats['total_words']:,}")
    print(f"Avg words/sentence: {stats['avg_words_per_sentence']:.1f}")
    print(f"Avg chars/sentence: {stats['avg_chars_per_sentence']:.1f}")
    print(f"Word range: {stats['min_words']}-{stats['max_words']}")
    print(f"Emoji count: {stats['emoji_count']}")
    print(f"Unique characters: {stats['unique_characters']}")
    
    return stats

# Usage
if __name__ == "__main__":
    # Analyze all datasets
    datasets = [
        ('data/processed/english_clean.txt', 'en'),
        ('data/processed/japanese_clean.txt', 'ja'),
    ]
    
    all_stats = []
    for file_path, lang in datasets:
        if Path(file_path).exists():
            stats = analyze_dataset(Path(file_path), lang)
            all_stats.append(stats)
    
    # Save stats
    with open('data/dataset_stats.json', 'w') as f:
        json.dump(all_stats, f, indent=2, ensure_ascii=False)
```

---

## 7. Recommended Data Sources

### Free Public Datasets

1. **English Casual**:
   - Twitter API (with proper authentication)
   - Reddit API (r/CasualConversation, r/AskReddit)
   - OpenSubtitles (movie dialogue)
   - Common Crawl (filtered for conversational text)

2. **Japanese Polite**:
   - JESC (Japanese-English Subtitle Corpus)
   - Tatoeba Project (sentence pairs)
   - JParaCrawl (web-crawled Japanese text)
   - Wikipedia Japanese (formal writing)

3. **Multilingual**:
   - OPUS (parallel corpora)
   - Tatoeba (multilingual sentences)
   - CC-100 (Common Crawl monolingual)

### Data Collection Checklist

- [ ] Collect 10,000+ English sentences
- [ ] Collect 10,000+ Japanese sentences
- [ ] Include 500+ emoji examples
- [ ] Add 200+ mixed language examples
- [ ] Clean and validate all data
- [ ] Remove duplicates
- [ ] Check for offensive content
- [ ] Verify emoji preservation
- [ ] Split train/val/test (80/10/10)
- [ ] Generate statistics report

---

## 8. Final Dataset Structure

```
data/
â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ english_casual.txt          # Raw English data
â”‚   â”œâ”€â”€ japanese_polite.txt         # Raw Japanese data
â”‚   â”œâ”€â”€ emoji_rich.txt              # Emoji examples
â”‚   â””â”€â”€ mixed_language.txt          # Code-switching
â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ english_clean.txt           # Cleaned English
â”‚   â”œâ”€â”€ japanese_clean.txt          # Cleaned Japanese
â”‚   â”œâ”€â”€ combined_train.txt          # Combined training set
â”‚   â”œâ”€â”€ validation.txt              # Validation set (10%)
â”‚   â””â”€â”€ test.txt                    # Test set (10%)
â””â”€â”€ dataset_stats.json              # Statistics report
```

---

## Next Steps

1. **Collect data** using the templates above
2. **Clean data** with the provided scripts
3. **Validate** using the analysis tools
4. **Train tokenizer** on combined dataset
5. **Train model** with the full dataset
6. **Evaluate** on test set

**Target**: 20,000+ sentences minimum for production-quality model!
