data:
  language: japanese
  max_sequence_length: 50
  train_file: data/processed/comprehensive_train.txt
model:
  dropout: 0.2
  embedding_dim: 128
  hidden_dim: 256
  num_layers: 1
  vocab_size: 32000
optimization:
  target_model_size_mb: 25
tokenizer:
  character_coverage: 0.9995
  model_type: unigram
  type: sentencepiece
  vocab_size: 32000
training:
  batch_size: 128
  early_stopping_patience: 5
  gradient_clip: 1.0
  learning_rate: 0.001
  log_interval: 1000
  num_epochs: 30
  save_dir: models/japanese
  val_split: 0.05
  validation_split: 0.05
